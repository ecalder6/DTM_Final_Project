{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reddit Neural Bot Trainer\n",
    "-----\n",
    "#### ToDo\n",
    "- Subredding embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b0aec606690e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAX_COMMENT_LENGTH = 20\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# x = tf.Variable([1.0, 2.0])\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(init)\n",
    "# v = sess.run(x)    \n",
    "# print(v) # will show you your variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "meta = pickle.load( open( \"metadata\", \"rb\" ) )\n",
    "\n",
    "vocab = meta[\"vocab\"]\n",
    "word_to_ix = meta[\"w2idx\"]\n",
    "ix_to_word = meta[\"idx2w\"]\n",
    "\n",
    "def to_eng(ids):\n",
    "    return ' '.join([ix_to_word[id] if id != 0 else '' for id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "proto_files = glob.glob('./*.tfrecords')\n",
    "random.shuffle(proto_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(proto_files)  #num_epochs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(\n",
    "  serialized_example,\n",
    "  # Defaults are not specified since both keys are required.\n",
    "  features={\n",
    "      #'subredddit_id': tf.FixedLenFeature([], tf.int64),\n",
    "      'question': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "      'answer': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "  })\n",
    "\n",
    "\n",
    "# normal_rv = tf.Variable( tf.truncated_normal([2,3],stddev = 0.1))\n",
    "\n",
    "# #initialize the variable\n",
    "# init_op = tf.global_variables_initializer()\n",
    "# print(normal_rv)\n",
    "# print(features[\"comment\"])\n",
    "\n",
    "# #run the graph\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init_op) #execute init_op\n",
    "#     #print the random values that we sample\n",
    "#     print (sess.run(normal_rv))\n",
    "#     print (sess.run(features[\"comment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_after_dequeue = 10000\n",
    "capacity = min_after_dequeue + 3 * BATCH_SIZE\n",
    "\n",
    "comment, replies = tf.train.shuffle_batch(\n",
    "    [features['question'], features['answer']],\n",
    "    batch_size=BATCH_SIZE, capacity=capacity, min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "SEQ_MAX_LEN = MAX_COMMENT_LENGTH\n",
    "RNN_HIDDEN_SIZE = 1024\n",
    "LAYERS = 3\n",
    "CHAR_EMB_SIZE = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "#SUBREDDIT_EMB_SIZE = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inner_cell = tf.contrib.rnn.BasicLSTMCell(RNN_HIDDEN_SIZE)\n",
    "enc_cell = tf.contrib.rnn.MultiRNNCell([inner_cell] * LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "char_embeddings = tf.get_variable(\"embedding\", [VOCAB_SIZE, CHAR_EMB_SIZE])\n",
    "emb_comment = tf.nn.embedding_lookup(char_embeddings, comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_, thought_vector = tf.nn.dynamic_rnn(\n",
    "    enc_cell, emb_comment, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reply_input = tf.concat(  # Add GO token to start\n",
    "    [tf.zeros(shape=(BATCH_SIZE, 1), dtype=tf.int64), replies[:, :SEQ_MAX_LEN-1]], axis=1)\n",
    "emb_reply_input = tf.nn.embedding_lookup(char_embeddings, reply_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dec_cell = tf.contrib.rnn.OutputProjectionWrapper(enc_cell, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\"):\n",
    "    dec_out, _ = tf.nn.dynamic_rnn(\n",
    "        dec_cell, emb_reply_input, initial_state=thought_vector, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=dec_out, labels=replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(xent, axis=[1])\n",
    "ave_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_1/rnn/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/cond/Merge:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/Squeeze:0\", shape=(100,), dtype=int64)\n",
      "Tensor(\"decoder_1/rnn/while/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/cond/Merge:0\", shape=(100, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_TEMP = 0.7\n",
    "\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    if cell_output is None:  # time == 0\n",
    "        next_cell_state = thought_vector  # state from the encoder\n",
    "        next_input = tf.zeros([BATCH_SIZE], dtype=tf.int64)  # GO symbol\n",
    "        next_input = tf.nn.embedding_lookup(char_embeddings, next_input)\n",
    "        emit_output = tf.zeros([], dtype=tf.int64)\n",
    "    else:\n",
    "        next_cell_state = cell_state\n",
    "        sample = tf.squeeze(tf.multinomial(cell_output / SAMPLE_TEMP, 1))\n",
    "        print(sample)\n",
    "        emb_sample = tf.nn.embedding_lookup(char_embeddings, sample)\n",
    "        next_input = emb_sample\n",
    "        emit_output = sample\n",
    "    elements_finished = time >= tf.constant(SEQ_MAX_LEN, shape=(BATCH_SIZE,))\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    print(next_input)\n",
    "    next_input = tf.cond(\n",
    "        finished,\n",
    "        lambda: tf.zeros([BATCH_SIZE, CHAR_EMB_SIZE], dtype=tf.float32),\n",
    "        lambda: next_input)\n",
    "    print(next_input)\n",
    "    next_loop_state = None\n",
    "    return elements_finished, next_input, next_cell_state, emit_output, next_loop_state\n",
    "\n",
    "with tf.variable_scope(\"decoder\", reuse=True):\n",
    "    outputs_ta, _, _ = tf.nn.raw_rnn(dec_cell, loop_fn, swap_memory=True)\n",
    "    sample = outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = tf.placeholder_with_default(LEARNING_RATE, [], name=\"lr\")\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(ave_loss, tvars), 1.0)\n",
    "optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Variables:\n",
      "====================\n",
      "        [20002, 128]  2560256 embedding:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "       [1024, 20002] 20482048 decoder/rnn/output_projection_wrapper/weights:0\n",
      "             [20002]    20002 decoder/rnn/output_projection_wrapper/biases:0\n",
      "Total trainable parameters: 66078498\n",
      "\n",
      "Other Varaibles:\n",
      "================\n",
      "        [20002, 128]  2560256 embedding/RMSProp:0\n",
      "        [20002, 128]  2560256 embedding/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "       [1024, 20002] 20482048 decoder/rnn/output_projection_wrapper/weights/RMSProp:0\n",
      "       [1024, 20002] 20482048 decoder/rnn/output_projection_wrapper/weights/RMSProp_1:0\n",
      "             [20002]    20002 decoder/rnn/output_projection_wrapper/biases/RMSProp:0\n",
      "             [20002]    20002 decoder/rnn/output_projection_wrapper/biases/RMSProp_1:0\n",
      "Total non-trainable parameters: 132156996\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def print_shapes():\n",
    "    train_vars = tf.trainable_variables()\n",
    "    \n",
    "    lines = ['']\n",
    "    lines.append('Trainable Variables:')\n",
    "    lines.append('====================')\n",
    "    total_params = 0\n",
    "    for var in train_vars:\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total trainable parameters: %d' % total_params)\n",
    "    \n",
    "    lines.append('')\n",
    "    lines.append('Other Varaibles:')\n",
    "    lines.append('================')\n",
    "    total_params = 0\n",
    "    for var in tf.global_variables():\n",
    "        if var in train_vars: continue\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total non-trainable parameters: %d' % total_params)\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print(print_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#coord.join(threads)\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT_PATH = './checkpoints/'\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "# latest_checkpoint = tf.train.latest_checkpoint(CHECKPOINT_PATH)\n",
    "# if latest_checkpoint:\n",
    "#     saver.restore(sess, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "198.071426392 ( 14.2878332298 )\t| 8.616 sec\n",
      "====================================\n",
      "looking forward to your tweets                --> logged shines arcade extending military dancer meghan further 4pcsset sidekick 100x 84 okie sample lacking plug sara deeply sez inhaled\n",
      "====================================\n",
      "====================================\n",
      "âš½ ï¸ and a few ðŸ» ðŸ»              --> egyptians grits makeover borrowing wasting bored brooo unfortunately ware 30k natalie rapping ontario tuition equality ðŸ¼ deluded goodness 925 simpsons\n",
      "====================================\n",
      "====================================\n",
      "ask me how i know ive lived in san francisco long enough         --> notified abstract eventual te sierra rider homosexuality condolences simon giggling study glasses shallows icon lt3 wnba geo arrest unlock closest\n",
      "====================================\n",
      "====================================\n",
      "make sure you add ice cubes to his unk            --> swollen explain facing holidays operating work inmates networking lgbt states enforcing atop region posture fbi mansplaining meek easier measurement absolutely\n",
      "====================================\n",
      "====================================\n",
      "wed love a key                 --> dreamforce wheeling compared snapshot aged vincent lip courses doctor planted dems excited float oven unrealistic cuff atrocity adrian worrying loaf\n",
      "====================================\n",
      "====================================\n",
      "you â€™ d be shocked how much google execs dislike music product in general       --> including knowledgeable midterm rental mandy introduces reyes household shirtless flat next australia outsource patriot diluted zika acknowledged lands nevermind outside\n",
      "====================================\n",
      "====================================\n",
      "congrats well deserved                  --> narratives careful switches outing reminiscing printers loudest sweatpants mics chickfila horror travel ix â›„ community suspicion humble mansion proves ploy\n",
      "====================================\n",
      "====================================\n",
      "have you got any we can climb in and hide           --> ilysm nfls whiff childish snorted thots perpetually seemingly staying ðŸ‡ª relying consuming flips fdny ðŸ‘´ opportunist kevin gope rodgers tribe\n",
      "====================================\n",
      "====================================\n",
      "did you see the random number generator example take a look should have everything you need good luck   --> shep megyn lineup pennsylvania spawn bucks 5th fanfic progression gtfo tsunami total dx congestion rants farms sail moes taunting idiocy\n",
      "====================================\n",
      "====================================\n",
      "for me unk unk the unk and unk dog unk           --> drunk protest admittedly craig nonpartisan closely certainty schmidt drops humans sipping honey mayhem stealing scrimmage shiiit fraternity improvised gigi arrogant\n",
      "====================================\n",
      "====================================\n",
      "youre a support network for delusional idiots who create a danger to public safety amp law enforcement carry on egg --> welcome jack sundays colleague html koran doug signal providence technologies homophobic yessir scarier suck nuh protested pinch tail thorough kappa\n",
      "====================================\n",
      "====================================\n",
      "burger king would rather let poor people starve to death than let them eat their food     --> collectors indirect altering ðŸ˜„ dodgers meatballs negligence hit cheerios visa ufc participating javascript cords stations crossover presumably fcking sheet disrupting\n",
      "====================================\n",
      "====================================\n",
      "thanks ðŸ¾ ðŸ‘ ðŸ¼ ðŸˆ                --> conjuring membership profits mentor lowell founder gp chappaqua tay etc hollow warnings wine interference insists ðŸ¤” systems amen clicked pitches\n",
      "====================================\n",
      "====================================\n",
      "every question answered unk did hillary raise the birther issue in the first place       --> ignite conversation legacy binder thief ðŸ˜ token politifact leads chico zealand a1 insinuating loosing okposo sonoma rating lovers reel engineers\n",
      "====================================\n",
      "====================================\n",
      "i love this                  --> bloom hyd channels highway brakes regina permit dawg sac enjoyed unicorns micro selling tmrw surprised mis mummy callaway fee pretending\n",
      "====================================\n",
      "====================================\n",
      "totally agree                   --> temper ideologies ðŸŒž antibiotics count daylight dramatic staging census time rampage bluffs 1989 pep taking fad tread waaay q towers\n",
      "====================================\n",
      "====================================\n",
      "unk wagon overdue time to make another unk unk w new faces         --> concluded barnes stafford polished pals chased thurs feminism festivals vancouver triangle mutuals indoctrinated erased responds sighs beck ðŸ‡© renew fandom\n",
      "====================================\n",
      "====================================\n",
      "its sad but business is business               --> evaluate hubs ranks literally earbuds sigma ebook kosher invest spacex salesforce ppr ff easing shea too haircut portrait mentors charge\n",
      "====================================\n",
      "====================================\n",
      "thankfully coming out of the worlds foremost millennial sponsored content website i have no unk of truth unk   --> illegals gretzky anarchy sen sherlock overall elder carried takeaway opposing baxter spoiler fascism shaping stinks predictable diaries sos bust birds\n",
      "====================================\n",
      "====================================\n",
      "how about taking notes on how he showed he respect and handled her ass       --> deploy larry pm environment wrigley ibm aloha hairline chall topic protesters retrospective cuomo huma girlfriend criminality high million israelites millenials\n",
      "====================================\n",
      "\n",
      "100\n",
      "197.95509201 ( 14.2794414781 )\t| 1.351 sec\n",
      "====================================\n",
      "thanks anyway for ur support                --> lt3 touchdowns tally high crying nikes costed sweaters bobble lions ðŸŒ¸ lessons christianity gd booking compound lex ministers ballot loud\n",
      "====================================\n",
      "====================================\n",
      "but thats how he rolls being the dumbass             --> parasites listens rev 1987 stumbles waivers uptown speak howie defenses sheet often modern yeahh wk dressed women claire plot user\n",
      "====================================\n",
      "====================================\n",
      "i mean its pretty mild as far as unk go nobody is banned or suspended and its what 2 games --> well judy objects sets insecurity trusting bortles busting racebaiting drone butthole tbqh reasoning apologist fairy godmother generating throw slam pull\n",
      "====================================\n",
      "====================================\n",
      "nah david brooks talked about football and thanksgiving fucking in the new york times       --> roommate clarification flag scammed stout villain a8 cheers opp mediocrity disaster enjoyable boycotting urban pew ominous spiral deals offices shut\n",
      "====================================\n",
      "====================================\n",
      "ive only had grits a handful of times and im appalled          --> crumbling 430 kindle notebook skates relay authorities dosnt empowered daddies cocktail illustrations loll fiancÃ© ðŸš½ longterm competes grammar loving slow\n",
      "====================================\n",
      "====================================\n",
      "let me just interrupt and say unk gets pretty unk online no balls unk       --> water raisin nostalgic strategically macos rodgers crypto 1970 wno firebase warning her continuously torturing rooftop imagination amend 2000s responds dignified\n",
      "====================================\n",
      "====================================\n",
      "i knew youd appreciate that dave               --> performers dates feeds handouts adderall btch athlete anthology mechanism sfo boner alcoholic gtgtgtgt motor idgi stagnant increasing pros empowerment judge\n",
      "====================================\n",
      "====================================\n",
      "you never need to sleep alone               --> haitians protect truthers surname ape feet sticking blades poem drown surging massively 924 facilitate kappa hundreds addition ten ðŸ˜— teaming\n",
      "====================================\n",
      "====================================\n",
      "lmao me amp my roomie a couple of years ago           --> opp pearl arrogance robbie essay pundits firewatch selleck entice colombian rooted threw stumble appropriating satisfied hatin clapping smite contour literacy\n",
      "====================================\n",
      "====================================\n",
      "i fully support this movement                --> summed frank devops ohh support finishes boy solely antitrump corrupted ideally choking harold anticipation revolting commercials uncertain squandered bread crystal\n",
      "====================================\n",
      "====================================\n",
      "can we bar him from more than just that            --> predicted elders shave shipment demonstrates respond unboxing ave dire dillon lyft laguardia forehead pete arcan ðŸ•· goodmorning 37 beep homework\n",
      "====================================\n",
      "====================================\n",
      "probably a bad idea to do it in unk            --> unfollowed realized dfs predebate trails handy sunnis moly peterson frankenstein shocker cool 60s wordplay ones gearing kneeling standard suit roast\n",
      "====================================\n",
      "====================================\n",
      "do you guys have actual talking points on this its a little embarrassing        --> legit downloads council ka demented adjusting 3500 tb dense operates bed antipolice flashback drill offenses enuff politely gaga created happening\n",
      "====================================\n",
      "====================================\n",
      "unk potato chips rule                 --> compassion relaxed kate hall brighton cucks spout ann meantime dumpling prettiest decor several amiright interviews soothing shade revolutionary developed notions\n",
      "====================================\n",
      "====================================\n",
      "normie journalists cant be bothered to understand chan humor            --> rush father scored kenzo felony dedicate stomach clerk rematch union windy distressing rofl filed stiffs lastly buff minnie mussolini winston\n",
      "====================================\n",
      "====================================\n",
      "wait you aren â€™ t in line right now ðŸ“± ðŸš¶ ðŸš¶ ðŸš¶ ðŸƒ       --> hillbots resulting thatd martian renee outage bank venus megan romo overpaid emotion howie groceries channel pbs tap intro aft excel\n",
      "====================================\n",
      "====================================\n",
      "trumps insistence on unk every perceived slight would be disastrous in a world leader       --> scale adjust hearing wld leopard trashy predatory avenues tote faint burden span fbis ah gray poetry unleashed 21st candy social\n",
      "====================================\n",
      "====================================\n",
      "they are different they don â€™ t contain devices but have its own type as far as i know  --> aggression midnight sent incest fruity commissions reich ofcourse hooking hahahahaha center gas bah graph hopping asleep faults holts decency spelled\n",
      "====================================\n",
      "====================================\n",
      "imagine tweeting a youtuber like he would actually respond cannot relate at all        --> modi handful athletics passport functional certain compromise thanking enabling executing ðŸµ z day confuse spending shyster mastermind hun chops leo\n",
      "====================================\n",
      "====================================\n",
      "its the perfect fall accessory unk               --> pairs headaches hogan northbound enuff thereof vans fooling hookah foremost stateside joan milk goldman contrary ion diane demographics blumenthal rooted\n",
      "====================================\n",
      "\n",
      "200\n",
      "159.150130768 ( 11.4802552208 )\t| 1.472 sec\n",
      "====================================\n",
      "wish i was there 2 hear this look forward unk it after living with 4 30yrs     --> horrific      is             \n",
      "====================================\n",
      "====================================\n",
      "jokes on u unk i read the 6 day eviction notice u got        --> overheating   this                \n",
      "====================================\n",
      "====================================\n",
      "stopped watching her                  --> i winner                  \n",
      "====================================\n",
      "====================================\n",
      "better than your foot                 -->   i twist         a       \n",
      "====================================\n",
      "====================================\n",
      "i mean what a dude starts as a celebrated unk stumbles into building a unk business     --> unk      unk   ok    the you     \n",
      "====================================\n",
      "====================================\n",
      "everybody unk trump is your master he washed your bird brain already         --> wage 1993   its      see         \n",
      "====================================\n",
      "====================================\n",
      "every representative on the main number tells me something different so that i no longer believe what they tell me --> â›½ wrong amnesia rendered                \n",
      "====================================\n",
      "====================================\n",
      "outstanding                    --> amp  cellular        unk         \n",
      "====================================\n",
      "====================================\n",
      "congratulations on your new shoe line best of luck ðŸ˜Š very very handsome picture of you ðŸ˜Š â¤ ï¸  -->  of   unk               \n",
      "====================================\n",
      "====================================\n",
      "i love my unk but i feel like that partnership would unk benefit        --> infinity the  ðŸ˜¨           who     \n",
      "====================================\n",
      "====================================\n",
      "this was a democrat set up but its ok she was rude and blind got water     --> dismantling  to    square             \n",
      "====================================\n",
      "====================================\n",
      "you would be fun to shoot with              --> for nah  a   that             \n",
      "====================================\n",
      "====================================\n",
      "dude i knowww                  --> teen youre  in    chalk            \n",
      "====================================\n",
      "====================================\n",
      "well i have to go home so lets get our priorities straight here        --> chalk                   \n",
      "====================================\n",
      "====================================\n",
      "very very slowly                  --> keeps                   \n",
      "====================================\n",
      "====================================\n",
      "haha i try skip munich go to unk             --> victory                   \n",
      "====================================\n",
      "====================================\n",
      "im so sorry i didnt know you care next time i will can you forgive me     --> probably  unk pokemon  motorcycles  is            \n",
      "====================================\n",
      "====================================\n",
      "yeah same that is so disgusting               --> recreate and  my unk  to unk            \n",
      "====================================\n",
      "====================================\n",
      "we use it in the slack app              --> lobby                   \n",
      "====================================\n",
      "====================================\n",
      "lunch isnt even a problem lunch is unk             --> reyes   nope on               \n",
      "====================================\n",
      "\n",
      "300\n",
      "92.8287355804 ( 6.69617782369 )\t| 1.618 sec\n",
      "====================================\n",
      "under unk the economics of my family has been greater than r         --> i to unk so like  doesnt for for           \n",
      "====================================\n",
      "====================================\n",
      "i dont blame you to take action              --> he and unk have clinton  team of unk           \n",
      "====================================\n",
      "====================================\n",
      "yes their next adventure is coming winter of 2017 ðŸ˜Š           --> im in   they    your           \n",
      "====================================\n",
      "====================================\n",
      "yeah i mean imagine if there was a racist who didnt wash his hands       --> i him has i an any    yesterday          \n",
      "====================================\n",
      "====================================\n",
      "of course its terrorist no common bad guy males sly targets civilians absurd        --> of apple never just  in people  let   it        \n",
      "====================================\n",
      "====================================\n",
      "i really should                  --> near  on if is in  a            \n",
      "====================================\n",
      "====================================\n",
      "plus how many women have that severe of stretch marks           --> an i unk you in  so             \n",
      "====================================\n",
      "====================================\n",
      "then it wouldnt be special now would it also are you truly prepared for every holiday to be year round --> unk heard you unk  to  night            \n",
      "====================================\n",
      "====================================\n",
      "at least you spelled karaoke right               --> a is even close   at    too         \n",
      "====================================\n",
      "====================================\n",
      "i will wait for evidence before conjecture              --> clear make better get unk  to of it on          \n",
      "====================================\n",
      "====================================\n",
      "that move was completely political to appease drivers no one thinks its better for transit riders     --> to unk would 100m  it              \n",
      "====================================\n",
      "====================================\n",
      "awesome singer reminds me of goth minister              --> can the unk soon are the  on            \n",
      "====================================\n",
      "====================================\n",
      "just add my skype unk                --> double evening if season the  in location            \n",
      "====================================\n",
      "====================================\n",
      "like i just wanted to watch this set and now i gotta spend time rolling my eyes you instead  --> this its has  it that   a           \n",
      "====================================\n",
      "====================================\n",
      "yes you will we will even have veterans onsite            --> i about  in  in a bumped   was         \n",
      "====================================\n",
      "====================================\n",
      "i dont see how thats possible               --> my is is that your what to way  but birthday         \n",
      "====================================\n",
      "====================================\n",
      "the family that owns it kept it for so long bcs of that its sickening      --> there  not do           lol     \n",
      "====================================\n",
      "====================================\n",
      "is that real                  --> you fun this first oh the  smh  unk          \n",
      "====================================\n",
      "====================================\n",
      "lmao im in a car in a short dress            --> just you unk for i unk version             \n",
      "====================================\n",
      "====================================\n",
      "go play with your friend george soros the deranged maniac           --> explosion i because worst was was and to her           \n",
      "====================================\n",
      "\n",
      "400\n",
      "91.8289234924 ( 6.62405662664 )\t| 1.641 sec\n",
      "====================================\n",
      "oh yeah and christie stopped eating donuts years ago            --> say he they the new a  anyway            \n",
      "====================================\n",
      "====================================\n",
      "ouch i â€™ d only ever get unk for insurance so deciding factor is coverage or price    --> this to for fun  from  dont of   the with       \n",
      "====================================\n",
      "====================================\n",
      "you seem to say that a lot              --> be in you any that   also            \n",
      "====================================\n",
      "====================================\n",
      "the gop has failed to call trump out on anything           --> read for unk literally the  the             \n",
      "====================================\n",
      "====================================\n",
      "this right here earns you a black belt in unk           --> to i unk a my               \n",
      "====================================\n",
      "====================================\n",
      "hahaha or maybe you just planted it here i â€™ m still not sure       --> unk my  well unk is a  bombing     a      \n",
      "====================================\n",
      "====================================\n",
      "at least we can also call them unk             --> why when apart its                \n",
      "====================================\n",
      "====================================\n",
      "do you need to borrow unk shes the bruce smith of mouse catching        --> i i not to  case down             \n",
      "====================================\n",
      "====================================\n",
      "the fact is that when hillary unk the deplorable number at 50 she thought she was being kind   --> love did all thats the the      this        \n",
      "====================================\n",
      "====================================\n",
      "congratulations you have the commercial memorized               --> happy buying you but i               \n",
      "====================================\n",
      "====================================\n",
      "ugh ðŸ‘ ðŸ¼ ugh ðŸ‘ ðŸ¼ ugh ðŸ‘ ðŸ¼            --> to unk its  the      good with        \n",
      "====================================\n",
      "====================================\n",
      "thanks andrew ðŸ‘Š                  --> i so yummy is unk didnt to             \n",
      "====================================\n",
      "====================================\n",
      "im kidding he probably just use his phone to mix he dont even got a laptop rn    --> just i to unk she of what             \n",
      "====================================\n",
      "====================================\n",
      "lmao they suck how did you get one             -->  i our in theres  today             \n",
      "====================================\n",
      "====================================\n",
      "lets keep em in the bay               --> unk what unk i on  is             \n",
      "====================================\n",
      "====================================\n",
      "greek casual right                  --> unk lol it one about               \n",
      "====================================\n",
      "====================================\n",
      "thats really corny ðŸ˜‚ ðŸ˜‚ ðŸ˜‚               --> unk life im to apple   was            \n",
      "====================================\n",
      "====================================\n",
      "whos racist what are you talking about              --> unk a and do unk now the from before           \n",
      "====================================\n",
      "====================================\n",
      "haha you sound like me what happened              --> unk do unk  in lost  in            \n",
      "====================================\n",
      "====================================\n",
      "no it is not done you and your slimy unk perpetrated bs for 5 years it will last wyou longer --> i of the on not               \n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "l_ave = b_ave = d_ave = 0\n",
    "\n",
    "UPDATE_EVERY = 100\n",
    "\n",
    "for step in range(500):\n",
    "    start_time = time.time()\n",
    "    _, l = sess.run([train_op, ave_loss], {\n",
    "        lr: 0.0001\n",
    "    })\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    l_ave += l\n",
    "    b_ave += l / SEQ_MAX_LEN / np.log(2.)\n",
    "    d_ave += duration\n",
    "    \n",
    "    #print(\"|\", end=\"\")\n",
    "    if step % UPDATE_EVERY == 0:\n",
    "        print()\n",
    "        l_ave = l_ave / UPDATE_EVERY if step else l_ave\n",
    "        b_ave = b_ave / UPDATE_EVERY if step else b_ave\n",
    "        d_ave = d_ave / UPDATE_EVERY if step else d_ave\n",
    "        \n",
    "        print(step)\n",
    "        print(l_ave, \"(\", b_ave, \")\\t|\", \"%.3f sec\" % d_ave)\n",
    "        c, r = sess.run([comment, sample])\n",
    "        for i in range(20):\n",
    "            print(\"====================================\")\n",
    "            print(to_eng(c[i]), \"-->\", to_eng(r[:, i]))\n",
    "            print(\"====================================\")\n",
    "\n",
    "        l_ave = b_ave = d_ave = 0\n",
    "print(\"DONE\")\n",
    "#         saver.save(sess, CHECKPOINT_PATH + \"checkpoint\", global_step=0)\n",
    "        #print('-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
