{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Neural Bot Trainer\n",
    "-----\n",
    "#### ToDo\n",
    "- Subredding embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_COMMENT_LENGTH = 20\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# x = tf.Variable([1.0, 2.0])\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(init)\n",
    "# v = sess.run(x)    \n",
    "# print(v) # will show you your variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "meta = pickle.load( open( \"metadata\", \"rb\" ) )\n",
    "\n",
    "vocab = meta[\"vocab\"]\n",
    "word_to_ix = meta[\"w2idx\"]\n",
    "ix_to_word = meta[\"idx2w\"]\n",
    "\n",
    "def to_eng(ids):\n",
    "    return ' '.join([ix_to_word[id] if id != 0 else '' for id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proto_files = glob.glob('./*.tfrecords')\n",
    "random.shuffle(proto_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(proto_files)  #num_epochs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(\n",
    "  serialized_example,\n",
    "  # Defaults are not specified since both keys are required.\n",
    "  features={\n",
    "      #'subredddit_id': tf.FixedLenFeature([], tf.int64),\n",
    "      'question': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "      'answer': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "  })\n",
    "\n",
    "\n",
    "# normal_rv = tf.Variable( tf.truncated_normal([2,3],stddev = 0.1))\n",
    "\n",
    "# #initialize the variable\n",
    "# init_op = tf.global_variables_initializer()\n",
    "# print(normal_rv)\n",
    "# print(features[\"comment\"])\n",
    "\n",
    "# #run the graph\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init_op) #execute init_op\n",
    "#     #print the random values that we sample\n",
    "#     print (sess.run(normal_rv))\n",
    "#     print (sess.run(features[\"comment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_after_dequeue = 10000\n",
    "capacity = min_after_dequeue + 3 * BATCH_SIZE\n",
    "\n",
    "comment, replies = tf.train.shuffle_batch(\n",
    "    [features['question'], features['answer']],\n",
    "    batch_size=BATCH_SIZE, capacity=capacity, min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "SEQ_MAX_LEN = MAX_COMMENT_LENGTH\n",
    "RNN_HIDDEN_SIZE = 1024\n",
    "LAYERS = 3\n",
    "CHAR_EMB_SIZE = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "#SUBREDDIT_EMB_SIZE = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inner_cell = tf.contrib.rnn.BasicLSTMCell(RNN_HIDDEN_SIZE)\n",
    "enc_cell = tf.contrib.rnn.MultiRNNCell([inner_cell] * LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_embeddings = tf.get_variable(\"embedding\", [VOCAB_SIZE, CHAR_EMB_SIZE])\n",
    "emb_comment = tf.nn.embedding_lookup(char_embeddings, comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, thought_vector = tf.nn.dynamic_rnn(\n",
    "    enc_cell, emb_comment, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reply_input = tf.concat(  # Add GO token to start\n",
    "    [tf.zeros(shape=(BATCH_SIZE, 1), dtype=tf.int64), replies[:, :SEQ_MAX_LEN-1]], axis=1)\n",
    "emb_reply_input = tf.nn.embedding_lookup(char_embeddings, reply_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_cell = tf.contrib.rnn.OutputProjectionWrapper(enc_cell, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\"):\n",
    "    dec_out, _ = tf.nn.dynamic_rnn(\n",
    "        dec_cell, emb_reply_input, initial_state=thought_vector, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=dec_out, labels=replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(xent, axis=[1])\n",
    "ave_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_1/rnn/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/cond/Merge:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/Squeeze:0\", shape=(100,), dtype=int64)\n",
      "Tensor(\"decoder_1/rnn/while/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/cond/Merge:0\", shape=(100, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_TEMP = 0.7\n",
    "\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    if cell_output is None:  # time == 0\n",
    "        next_cell_state = thought_vector  # state from the encoder\n",
    "        next_input = tf.zeros([BATCH_SIZE], dtype=tf.int64)  # GO symbol\n",
    "        next_input = tf.nn.embedding_lookup(char_embeddings, next_input)\n",
    "        emit_output = tf.zeros([], dtype=tf.int64)\n",
    "    else:\n",
    "        next_cell_state = cell_state\n",
    "        sample = tf.squeeze(tf.multinomial(cell_output / SAMPLE_TEMP, 1))\n",
    "        print(sample)\n",
    "        emb_sample = tf.nn.embedding_lookup(char_embeddings, sample)\n",
    "        next_input = emb_sample\n",
    "        emit_output = sample\n",
    "    elements_finished = time >= tf.constant(SEQ_MAX_LEN, shape=(BATCH_SIZE,))\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    print(next_input)\n",
    "    next_input = tf.cond(\n",
    "        finished,\n",
    "        lambda: tf.zeros([BATCH_SIZE, CHAR_EMB_SIZE], dtype=tf.float32),\n",
    "        lambda: next_input)\n",
    "    print(next_input)\n",
    "    next_loop_state = None\n",
    "    return elements_finished, next_input, next_cell_state, emit_output, next_loop_state\n",
    "\n",
    "with tf.variable_scope(\"decoder\", reuse=True):\n",
    "    outputs_ta, _, _ = tf.nn.raw_rnn(dec_cell, loop_fn, swap_memory=True)\n",
    "    sample = outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.placeholder_with_default(LEARNING_RATE, [], name=\"lr\")\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(ave_loss, tvars), 1.0)\n",
    "optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Variables:\n",
      "====================\n",
      "         [6000, 128]   768000 embedding:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "        [1024, 6000]  6144000 decoder/rnn/output_projection_wrapper/weights:0\n",
      "              [6000]     6000 decoder/rnn/output_projection_wrapper/biases:0\n",
      "Total trainable parameters: 49934192\n",
      "\n",
      "Other Varaibles:\n",
      "================\n",
      "         [6000, 128]   768000 embedding/RMSProp:0\n",
      "         [6000, 128]   768000 embedding/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [1024, 6000]  6144000 decoder/rnn/output_projection_wrapper/weights/RMSProp:0\n",
      "        [1024, 6000]  6144000 decoder/rnn/output_projection_wrapper/weights/RMSProp_1:0\n",
      "              [6000]     6000 decoder/rnn/output_projection_wrapper/biases/RMSProp:0\n",
      "              [6000]     6000 decoder/rnn/output_projection_wrapper/biases/RMSProp_1:0\n",
      "Total non-trainable parameters: 99868384\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def print_shapes():\n",
    "    train_vars = tf.trainable_variables()\n",
    "    \n",
    "    lines = ['']\n",
    "    lines.append('Trainable Variables:')\n",
    "    lines.append('====================')\n",
    "    total_params = 0\n",
    "    for var in train_vars:\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total trainable parameters: %d' % total_params)\n",
    "    \n",
    "    lines.append('')\n",
    "    lines.append('Other Varaibles:')\n",
    "    lines.append('================')\n",
    "    total_params = 0\n",
    "    for var in tf.global_variables():\n",
    "        if var in train_vars: continue\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total non-trainable parameters: %d' % total_params)\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print(print_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#coord.join(threads)\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = './checkpoints/'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "latest_checkpoint = tf.train.latest_checkpoint(CHECKPOINT_PATH)\n",
    "if latest_checkpoint:\n",
    "    saver.restore(sess, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "434.943847656 ( 12.5498266416 )\t| 8.880 sec\n",
      "my bike has been a fun little project                                           --> fan respect hashtag founder curious bomber attention users tx loop ran looked ðŸ˜ðŸ˜ up install leave season avenue gf father any cafe planet burns always ship album retire north surprise orlando swear bucket headline robert test tank howard individual appear stopping computer perfectly developer bra gay root say insult lost\n",
      "that sad moment when you realize you never really knew                                         --> appear dems change 55 backed issues besides establishment mo inviting celebrity addressing understands updates keith hoes disease come poverty plain involved hosting full tinder whack soccer established attractive collapse extension will essential itself yards movie nuts horrible outs coke counting corporations alternative congrats heal startup any just pushed reform circus\n",
      "there are 50 coaches on the unk but no clock guy never made sense to me                                   --> pressed stands pack left call earn lyft disrespect pres episodes inspiring 85 legacy duty character ugh boys hypocrite guys endorsing choosing pile rush lose reason rex dollars 25 shows ðŸ™ˆ yesterday crooked mommy aspect credit students farmers dodgers unity opened chasing filed masters manager raising problem urban donation slave what\n",
      "woke up to a cubs 2016 unk division unk going to be a unk lift this morning isnt it                                --> laughing sony hence upload devices workout rise corn ðŸ™„ afternoon happiness magic meanwhile talks hilarious immigration refuse rachel leak interests wedding vc wells developer nap receiving enter attacks generally controlled auto train position health princess  king killing let deeper elementary proposal werent whatâ€™s folks handled jobs busted gained sos\n",
      "straight people news facebook live videos doing unk to raise awareness of unk                                      --> myth soooo hm pass highlights bump movie arena tho seattle juneonnabeat arent ðŸ¤“ wow emmys ladies hypocrisy coffee destroying alarm according less punch suggestion tons chose uk disgusted senior notes projects situations senator ourselves describe television annual conversation concert sat handling imma iâ€™d hands fifa steak reform dumb plot troll\n",
      "trump yes i now agree that guns kill people but also space aliens kill people as we learned in the documentary star wars                            --> regarding lee democrat cousin heat quiz valley snake werent toronto brad noticed stranger terrorist follows vc repeating fee pet suggest arm sir ðŸ™ exposure documents sort linked work officially chances baltimore jets finest passing talk growing fallen george surprisingly unfit wireless water vegan door tmobile wealth year homecoming priority crossing\n",
      "also on unk trump jr says unk attacked ivanka trump read trumps old quotes ivanka suggested she was making them up                              --> separate 85 protesting russia wld screen lead leading iron bastard inviting confidence attacking dumbest existing tons thread nose offense sentiment calendar take confused waiting apples polls capital credibility explosion 9th 4th stock eggs needed format outcome finna intellectual closet freak rally climate empire wit quite u building rain mile iâ€™ll\n",
      "no need for insults lil homie unk from the unk                                         --> affair spray ridiculous farmers guests trips light hah grey pres sides queer remain reid medical it predicted try florida pls drug outcome armed resort younger buildings 20 gym bar witness amendment pick scene mexican attitude oracle returned settled bloody advertising mf pile hook raise laws kelly pres wouldnt im cooking\n",
      "problem is corporations dont actually function in a free market but im done here  feel free to side with unk                              --> raid marshall whom worlds cruel intelligence politician 400 female shoes delays spot theory machines worthy back convince bae dudes scotus nowhere goodbye era your proper views 2b productive picking across viewers injustice pulling dropping pen better kitty agreement corporate coffee pic complicated florida realized stays aid trail sounding stone inbox\n",
      "i am finally going to be at peace                                           --> china brief shield trail lights effectively secretly msg platforms prevent majority suicide unit tie colorado jets tastes urself laid pitcher string comments communist washington resources controls didnâ€™t hold extremely scenario would sneak used powerful applies steven stay closed claiming preview treasure males united calendar t ho fine supremacy ðŸ˜ðŸ˜ feels\n",
      "watching unk                                                 --> happen causes ðŸ™‚ app jay jumping bull outside giant ok louis via except mirror sheep lunch girls fears outfit bastard powell spaces republican crash sox eight helps reporting doing baby tweeted theyâ€™re competition business policy biased welcome gross ankle fans bummed jungleboy aa understood suggested fill library logical loser commentary\n",
      "more voters said trump would bring real change while clinton was seen as having a better temperament for the job                               --> lean â¤ï¸â¤ï¸â¤ï¸ assume impression painful pee continues wear johnson ray beer sen khan jesus strategy reference kim jordan bucket interested figures fights insight haha feels 34 visual wednesday planning irl snowden included bathroom full stories candidate delete suffering articles dishonest chick purpose sits cali cubs load basket opposition gave w\n",
      "not sure what this means but just look at this                                         --> jobs observation purchase id humor ready lawyers whining shared offer care immigrants pulled rational cleveland humans harambe united movement bottom ugh france unarmed sm noah as garage unlikely neighborhood cunt supporters ugly view listened hispanic increase fought sells howard l lineup ts db parts shout david belt information warming â˜ºï¸\n",
      "unk of wine for 15 with                                             --> rick jam juice among cult richard foundation thrown mans ankle emotion loans semester sounding back decides os difference upgrade jess job compliment 08 bored legal anthem your file chill cities ðŸ˜‚ vikings at hoboken drink honey rb allow trying supporters girlfriend going 1000 magical clueless rhetoric teacher priorities progressive michigan\n",
      "hey do you have any gifs or video of our old favorite monkey the unk one pre harambe i need him today                             --> than americans get parks chrome once wrestling africa kill garbage thinks st forum hoping pass failing barely wayne belief ðŸ˜Ž ðŸ¤”ðŸ¤” respected reached conclusions tinder upgrade disturbing press highlight copies pop flights ave color anything backing list indeed symbol gem collins immigration explode ðŸ’‹ marvel frustrated terrifying youll homework pneumonia\n",
      "very powerful unk short film directed by  definitely worth a watch                                       --> dogs ignore after cbs billionaire noah stomach cocaine international forces desperate tricky invest ruin celebrating aesthetic definitely retail confidence holt jump revenue warmonger exchange reasons comfort rad delayed range join entirely chip horrible thankfully mouse earn petty figured refund sarah make excited possible awe mama reminded ðŸ¤”ðŸ¤”ðŸ¤” posted n endorses\n",
      "help video unk programs u would recommend 4 unk files unk unk camera unk unk off mid record                                 --> hunter shots opponent root potential miles anymore zombie hanging great ghost transparency border surprising acceptable â¤ subtle tyler browns threatened broad rice panel rosie prop later treating gonna custody hearts example religion century oppose duh chargers nj known confident goddamn ears pumpkin shade beating russians too condolences hs relations brilliant\n",
      "all i really care about right now is banana coffee milk                                        --> choose snap thing loaded nfl friendly sf pepe glasses freshman wall view jfk stunning republic aight bizarre touched gotta naturally weigh bart baked chiefs destroy report truck republican accent brains bigots js analogy 99 prob yell grad jumped iowa relative recent veto blowing blunt collection shy mix lbs set imo\n",
      "good morning                                                 --> grocery charges snapchat ðŸ¤— mocking financial denial bots campaigns puppet jumped happy providing headed rise partners pokemon death technically force mystery dodgers child disgusting urban bucks november fb injury cheers denial companies toâ€¦ edit d actively tmrw section client integrity kyle learning called tf scheduled stuff positive wealthy 2x print\n",
      "patient voices not unk included at medical unk says                                          --> hint hispanic tail century eggs bear thankful grow natural nite bruce individual reading irresponsible disgusted celebrity 2b russia educated 9th selling opposed property threatening hoax cc selling go seemed reminds wherever emergency looking iran adds saints determine polish expression passion think get reply stop serious highly trans tonights rush rocks\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of ./checkpoints/checkpoint doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-fd0ee441b6f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0ml_ave\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_ave\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_ave\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCHECKPOINT_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"checkpoint\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;31m#print('-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ericc\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m       raise ValueError(\n\u001b[1;32m-> 1354\u001b[1;33m           \"Parent directory of {} doesn't exist, can't save.\".format(save_path))\n\u001b[0m\u001b[0;32m   1355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parent directory of ./checkpoints/checkpoint doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "l_ave = b_ave = d_ave = 0\n",
    "\n",
    "UPDATE_EVERY = 100\n",
    "\n",
    "for step in range(500):\n",
    "    start_time = time.time()\n",
    "    _, l = sess.run([train_op, ave_loss], {\n",
    "        lr: 0.0001\n",
    "    })\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    l_ave += l\n",
    "    b_ave += l / SEQ_MAX_LEN / np.log(2.)\n",
    "    d_ave += duration\n",
    "    \n",
    "    #print(\"|\", end=\"\")\n",
    "    if step % UPDATE_EVERY == 0:\n",
    "        print()\n",
    "        l_ave = l_ave / UPDATE_EVERY if step else l_ave\n",
    "        b_ave = b_ave / UPDATE_EVERY if step else b_ave\n",
    "        d_ave = d_ave / UPDATE_EVERY if step else d_ave\n",
    "        \n",
    "        print(step)\n",
    "        print(l_ave, \"(\", b_ave, \")\\t|\", \"%.3f sec\" % d_ave)\n",
    "        c, r = sess.run([comment, sample])\n",
    "        for i in range(20):\n",
    "            print(to_eng(c[i]), \"-->\", to_eng(r[:, i]))\n",
    "\n",
    "        l_ave = b_ave = d_ave = 0\n",
    "        print()\n",
    "        saver.save(sess, CHECKPOINT_PATH + \"checkpoint\", global_step=0)\n",
    "        #print('-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
