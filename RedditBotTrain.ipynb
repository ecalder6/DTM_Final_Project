{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Neural Bot Trainer\n",
    "-----\n",
    "#### ToDo\n",
    "- Subredding embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_COMMENT_LENGTH = 20\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# x = tf.Variable([1.0, 2.0])\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(init)\n",
    "# v = sess.run(x)    \n",
    "# print(v) # will show you your variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "meta = pickle.load( open( \"metadata\", \"rb\" ) )\n",
    "\n",
    "vocab = meta[\"vocab\"]\n",
    "word_to_ix = meta[\"w2idx\"]\n",
    "ix_to_word = meta[\"idx2w\"]\n",
    "\n",
    "def to_eng(ids):\n",
    "    return ' '.join([ix_to_word[id] if id != 0 else '' for id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proto_files = glob.glob('./*.tfrecords')\n",
    "random.shuffle(proto_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(proto_files)  #num_epochs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(\n",
    "  serialized_example,\n",
    "  # Defaults are not specified since both keys are required.\n",
    "  features={\n",
    "      #'subredddit_id': tf.FixedLenFeature([], tf.int64),\n",
    "      'question': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "      'answer': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "  })\n",
    "\n",
    "\n",
    "# normal_rv = tf.Variable( tf.truncated_normal([2,3],stddev = 0.1))\n",
    "\n",
    "# #initialize the variable\n",
    "# init_op = tf.global_variables_initializer()\n",
    "# print(normal_rv)\n",
    "# print(features[\"comment\"])\n",
    "\n",
    "# #run the graph\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init_op) #execute init_op\n",
    "#     #print the random values that we sample\n",
    "#     print (sess.run(normal_rv))\n",
    "#     print (sess.run(features[\"comment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_after_dequeue = 10000\n",
    "capacity = min_after_dequeue + 3 * BATCH_SIZE\n",
    "\n",
    "comment, replies = tf.train.shuffle_batch(\n",
    "    [features['question'], features['answer']],\n",
    "    batch_size=BATCH_SIZE, capacity=capacity, min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "SEQ_MAX_LEN = MAX_COMMENT_LENGTH\n",
    "RNN_HIDDEN_SIZE = 1024\n",
    "LAYERS = 3\n",
    "CHAR_EMB_SIZE = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "#SUBREDDIT_EMB_SIZE = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inner_cell = tf.contrib.rnn.BasicLSTMCell(RNN_HIDDEN_SIZE)\n",
    "enc_cell = tf.contrib.rnn.MultiRNNCell([inner_cell] * LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_embeddings = tf.get_variable(\"embedding\", [VOCAB_SIZE, CHAR_EMB_SIZE])\n",
    "emb_comment = tf.nn.embedding_lookup(char_embeddings, comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, thought_vector = tf.nn.dynamic_rnn(\n",
    "    enc_cell, emb_comment, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reply_input = tf.concat(  # Add GO token to start\n",
    "    [tf.zeros(shape=(BATCH_SIZE, 1), dtype=tf.int64), replies[:, :SEQ_MAX_LEN-1]], axis=1)\n",
    "emb_reply_input = tf.nn.embedding_lookup(char_embeddings, reply_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_cell = tf.contrib.rnn.OutputProjectionWrapper(enc_cell, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\"):\n",
    "    dec_out, _ = tf.nn.dynamic_rnn(\n",
    "        dec_cell, emb_reply_input, initial_state=thought_vector, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=dec_out, labels=replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(xent, axis=[1])\n",
    "ave_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_1/rnn/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/cond/Merge:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/Squeeze:0\", shape=(100,), dtype=int64)\n",
      "Tensor(\"decoder_1/rnn/while/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/cond/Merge:0\", shape=(100, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_TEMP = 0.7\n",
    "\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    if cell_output is None:  # time == 0\n",
    "        next_cell_state = thought_vector  # state from the encoder\n",
    "        next_input = tf.zeros([BATCH_SIZE], dtype=tf.int64)  # GO symbol\n",
    "        next_input = tf.nn.embedding_lookup(char_embeddings, next_input)\n",
    "        emit_output = tf.zeros([], dtype=tf.int64)\n",
    "    else:\n",
    "        next_cell_state = cell_state\n",
    "        sample = tf.squeeze(tf.multinomial(cell_output / SAMPLE_TEMP, 1))\n",
    "        print(sample)\n",
    "        emb_sample = tf.nn.embedding_lookup(char_embeddings, sample)\n",
    "        next_input = emb_sample\n",
    "        emit_output = sample\n",
    "    elements_finished = time >= tf.constant(SEQ_MAX_LEN, shape=(BATCH_SIZE,))\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    print(next_input)\n",
    "    next_input = tf.cond(\n",
    "        finished,\n",
    "        lambda: tf.zeros([BATCH_SIZE, CHAR_EMB_SIZE], dtype=tf.float32),\n",
    "        lambda: next_input)\n",
    "    print(next_input)\n",
    "    next_loop_state = None\n",
    "    return elements_finished, next_input, next_cell_state, emit_output, next_loop_state\n",
    "\n",
    "with tf.variable_scope(\"decoder\", reuse=True):\n",
    "    outputs_ta, _, _ = tf.nn.raw_rnn(dec_cell, loop_fn, swap_memory=True)\n",
    "    sample = outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.placeholder_with_default(LEARNING_RATE, [], name=\"lr\")\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(ave_loss, tvars), 1.0)\n",
    "optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Variables:\n",
      "====================\n",
      "         [6002, 128]   768256 embedding:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "        [1024, 6002]  6146048 decoder/rnn/output_projection_wrapper/weights:0\n",
      "              [6002]     6002 decoder/rnn/output_projection_wrapper/biases:0\n",
      "Total trainable parameters: 49936498\n",
      "\n",
      "Other Varaibles:\n",
      "================\n",
      "         [6002, 128]   768256 embedding/RMSProp:0\n",
      "         [6002, 128]   768256 embedding/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [1024, 6002]  6146048 decoder/rnn/output_projection_wrapper/weights/RMSProp:0\n",
      "        [1024, 6002]  6146048 decoder/rnn/output_projection_wrapper/weights/RMSProp_1:0\n",
      "              [6002]     6002 decoder/rnn/output_projection_wrapper/biases/RMSProp:0\n",
      "              [6002]     6002 decoder/rnn/output_projection_wrapper/biases/RMSProp_1:0\n",
      "Total non-trainable parameters: 99872996\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def print_shapes():\n",
    "    train_vars = tf.trainable_variables()\n",
    "    \n",
    "    lines = ['']\n",
    "    lines.append('Trainable Variables:')\n",
    "    lines.append('====================')\n",
    "    total_params = 0\n",
    "    for var in train_vars:\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total trainable parameters: %d' % total_params)\n",
    "    \n",
    "    lines.append('')\n",
    "    lines.append('Other Varaibles:')\n",
    "    lines.append('================')\n",
    "    total_params = 0\n",
    "    for var in tf.global_variables():\n",
    "        if var in train_vars: continue\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total non-trainable parameters: %d' % total_params)\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print(print_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#coord.join(threads)\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT_PATH = './checkpoints/'\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "# latest_checkpoint = tf.train.latest_checkpoint(CHECKPOINT_PATH)\n",
    "# if latest_checkpoint:\n",
    "#     saver.restore(sess, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "173.995620728 ( 12.551130958 )\t| 7.328 sec\n",
      "====================================\n",
      "just ask him out already                --> u bra minority about regularly minorities notion replace neighbors image contact ðŸ¾ treatment feed holding oops wasnt turns drag ps\n",
      "====================================\n",
      "====================================\n",
      "sure thing loved the talk is it will it be online anywhere         --> diverse reach sometimes cracked shelby digital thanks altright hook band like age stopandfrisk peres sport chain entry artists vr penalty\n",
      "====================================\n",
      "====================================\n",
      "no doubt unk native muslim obama was born in kenya           --> racism judging ðŸ˜“ cartoon losers heat paying candidate â˜¹ ian flower integrity kno towards cycle brunch degrees christie anthem featured\n",
      "====================================\n",
      "====================================\n",
      "i will take a listen                --> surface ðŸ™‚ appreciate department arguments raising maybe keynote holes ton military advocate collapse tuesday fr motivation dem demo april disagree\n",
      "====================================\n",
      "====================================\n",
      "love the style love the message love the shirt i gotta wear this nj as a promo for   --> main sharp subway assault product bloody disappointment jeez addressed irrelevant goodnight latest away falling mistakes remarks to 10k recap exciting\n",
      "====================================\n",
      "====================================\n",
      "i miss the part where he unk his religion monster           --> win which awards men jump debate moments duke lane demo wars fall shortly travel ðŸ’€ danny stood hahahah prop ðŸ‘‰\n",
      "====================================\n",
      "====================================\n",
      "oh amp first day of school ðŸ˜…              --> afford ground albums receive stream fully seems kno republican character insult some nothing 0 kevin islam attending transit qb âœŒ\n",
      "====================================\n",
      "====================================\n",
      "lol nah i â€™ m not even listening to music spoken podcasts         --> 70s consequences careful blumenthal mobile 11 monica everyones hate allegedly accepted puppy coast demo legend ðŸ‘» prolly jazz inner democrats\n",
      "====================================\n",
      "====================================\n",
      "that looks like a crew that deserves a unk unk           --> beans wealth fb nap lick grace embrace finishing alcohol speaker history iowa network chad situation compare population explanation eggs tru\n",
      "====================================\n",
      "====================================\n",
      "we recommend calling the store                --> g ohio addicted lip db course average slaves others host pushed cleaning secret sex straight paid seek target bizarre christian\n",
      "====================================\n",
      "====================================\n",
      "good excited for the weekend job searching really takes a unk on you        --> importance stopped travel realized youve theyll literal reads era kno slander kaepernick bob arrested mr womans still rude camp fiction\n",
      "====================================\n",
      "====================================\n",
      "im literally getting unk unk bc i miss you            --> old lab upper fly glorious finishing alright now birther wentz opportunity nerve somewhere annual differently search street tried island v\n",
      "====================================\n",
      "====================================\n",
      "islam is a unk on humanity               --> startup scientific appearance individual talkin have conservatives killed difficult solid exchange missed minds systems w christians freshman everyone hannity t\n",
      "====================================\n",
      "====================================\n",
      "that fucking hair                  --> stats fire tough reveal radio escape typo clear rudy task health drama coast wealthy cardinals statements victory talkin controls saints\n",
      "====================================\n",
      "====================================\n",
      "i can only hear no step on unk being unk by james unk good god      --> bot learn vet ðŸ˜‹ link remains boost posts conscience outta profile replying constitution dust ðŸ blew peanut injustice spit headphone\n",
      "====================================\n",
      "====================================\n",
      "is the coverage now turning to whether or not clinton started birtherism         --> stick her donations outrageous baby so deplorables liberals hypocrisy targeted andy tip discussion desperate consider necessarily claiming trap deplorable narrow\n",
      "====================================\n",
      "====================================\n",
      "she already got asked friday                --> os runs pointed delay motivation dressed enjoyed 99 minus idiot ðŸ‘‹ received hits nominated msg sentence income idiot flight faves\n",
      "====================================\n",
      "====================================\n",
      "shut yo bitch ass up                --> experts cheer animation â™¥ pet android president promote hurt sides influence laws illness therapy pretending campus bodies proposed introducing supporting\n",
      "====================================\n",
      "====================================\n",
      "such good dudes right here                --> encourage de suggests cd yea boobs crying settle jake 1000 guarantee movie vote grind saying heaven whens food storage hi\n",
      "====================================\n",
      "====================================\n",
      "half basket of deplorable not politically correct but unk true country is embarrassed friends unk terrorists happy    --> ðŸ’… pleased sir systems awe smart cowboys gm jon call stated up year deny wondering til skittles wa films somewhat\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "l_ave = b_ave = d_ave = 0\n",
    "\n",
    "UPDATE_EVERY = 100\n",
    "\n",
    "for step in range(500):\n",
    "    start_time = time.time()\n",
    "    _, l = sess.run([train_op, ave_loss], {\n",
    "        lr: 0.0001\n",
    "    })\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    l_ave += l\n",
    "    b_ave += l / SEQ_MAX_LEN / np.log(2.)\n",
    "    d_ave += duration\n",
    "    \n",
    "    #print(\"|\", end=\"\")\n",
    "    if step % UPDATE_EVERY == 0:\n",
    "        print()\n",
    "        l_ave = l_ave / UPDATE_EVERY if step else l_ave\n",
    "        b_ave = b_ave / UPDATE_EVERY if step else b_ave\n",
    "        d_ave = d_ave / UPDATE_EVERY if step else d_ave\n",
    "        \n",
    "        print(step)\n",
    "        print(l_ave, \"(\", b_ave, \")\\t|\", \"%.3f sec\" % d_ave)\n",
    "        c, r = sess.run([comment, sample])\n",
    "        for i in range(20):\n",
    "            print(\"====================================\")\n",
    "            print(to_eng(c[i]), \"-->\", to_eng(r[:, i]))\n",
    "            print(\"====================================\")\n",
    "\n",
    "        l_ave = b_ave = d_ave = 0\n",
    "print(\"DONE\")\n",
    "#         saver.save(sess, CHECKPOINT_PATH + \"checkpoint\", global_step=0)\n",
    "        #print('-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
