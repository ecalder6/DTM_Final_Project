{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reddit Neural Bot Trainer\n",
    "-----\n",
    "#### ToDo\n",
    "- Subredding embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAX_COMMENT_LENGTH = 50\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# x = tf.Variable([1.0, 2.0])\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(init)\n",
    "# v = sess.run(x)    \n",
    "# print(v) # will show you your variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "meta = pickle.load( open( \"metadata.pkl\", \"rb\" ) )\n",
    "\n",
    "vocab = meta[\"vocab\"]\n",
    "char_to_ix = meta[\"w2idx\"]\n",
    "ix_to_char = meta[\"idx2w\"]\n",
    "\n",
    "def to_eng(ids):\n",
    "    return ''.join([ix_to_char[id] if id != 0 else '' for id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "proto_files = glob.glob('./*.tfrecords')\n",
    "random.shuffle(proto_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(proto_files)  #num_epochs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(\n",
    "  serialized_example,\n",
    "  # Defaults are not specified since both keys are required.\n",
    "  features={\n",
    "      #'subredddit_id': tf.FixedLenFeature([], tf.int64),\n",
    "      'question': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "      'answer': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "  })\n",
    "\n",
    "\n",
    "# normal_rv = tf.Variable( tf.truncated_normal([2,3],stddev = 0.1))\n",
    "\n",
    "# #initialize the variable\n",
    "# init_op = tf.global_variables_initializer()\n",
    "# print(normal_rv)\n",
    "# print(features[\"comment\"])\n",
    "\n",
    "# #run the graph\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init_op) #execute init_op\n",
    "#     #print the random values that we sample\n",
    "#     print (sess.run(normal_rv))\n",
    "#     print (sess.run(features[\"comment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_after_dequeue = 10000\n",
    "capacity = min_after_dequeue + 3 * BATCH_SIZE\n",
    "\n",
    "comment, replies = tf.train.shuffle_batch(\n",
    "    [features['question'], features['answer']],\n",
    "    batch_size=BATCH_SIZE, capacity=capacity, min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "SEQ_MAX_LEN = MAX_COMMENT_LENGTH\n",
    "RNN_HIDDEN_SIZE = 1024\n",
    "LAYERS = 3\n",
    "CHAR_EMB_SIZE = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "#SUBREDDIT_EMB_SIZE = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inner_cell = tf.contrib.rnn.BasicLSTMCell(RNN_HIDDEN_SIZE)\n",
    "enc_cell = tf.contrib.rnn.MultiRNNCell([inner_cell] * LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "char_embeddings = tf.get_variable(\"embedding\", [VOCAB_SIZE, CHAR_EMB_SIZE])\n",
    "emb_comment = tf.nn.embedding_lookup(char_embeddings, comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_, thought_vector = tf.nn.dynamic_rnn(\n",
    "    enc_cell, emb_comment, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reply_input = tf.concat(  # Add GO token to start\n",
    "    [tf.zeros(shape=(BATCH_SIZE, 1), dtype=tf.int64), replies[:, :SEQ_MAX_LEN-1]], axis=1)\n",
    "emb_reply_input = tf.nn.embedding_lookup(char_embeddings, reply_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dec_cell = tf.contrib.rnn.OutputProjectionWrapper(enc_cell, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\"):\n",
    "    dec_out, _ = tf.nn.dynamic_rnn(\n",
    "        dec_cell, emb_reply_input, initial_state=thought_vector, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=dec_out, labels=replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(xent, axis=[1])\n",
    "ave_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_1/rnn/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/cond/Merge:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/Squeeze:0\", shape=(100,), dtype=int64)\n",
      "Tensor(\"decoder_1/rnn/while/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/cond/Merge:0\", shape=(100, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_TEMP = 0.7\n",
    "\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    if cell_output is None:  # time == 0\n",
    "        next_cell_state = thought_vector  # state from the encoder\n",
    "        next_input = tf.zeros([BATCH_SIZE], dtype=tf.int64)  # GO symbol\n",
    "        next_input = tf.nn.embedding_lookup(char_embeddings, next_input)\n",
    "        emit_output = tf.zeros([], dtype=tf.int64)\n",
    "    else:\n",
    "        next_cell_state = cell_state\n",
    "        sample = tf.squeeze(tf.multinomial(cell_output / SAMPLE_TEMP, 1))\n",
    "        print(sample)\n",
    "        emb_sample = tf.nn.embedding_lookup(char_embeddings, sample)\n",
    "        next_input = emb_sample\n",
    "        emit_output = sample\n",
    "    elements_finished = time >= tf.constant(SEQ_MAX_LEN, shape=(BATCH_SIZE,))\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    print(next_input)\n",
    "    next_input = tf.cond(\n",
    "        finished,\n",
    "        lambda: tf.zeros([BATCH_SIZE, CHAR_EMB_SIZE], dtype=tf.float32),\n",
    "        lambda: next_input)\n",
    "    print(next_input)\n",
    "    next_loop_state = None\n",
    "    return elements_finished, next_input, next_cell_state, emit_output, next_loop_state\n",
    "\n",
    "with tf.variable_scope(\"decoder\", reuse=True):\n",
    "    outputs_ta, _, _ = tf.nn.raw_rnn(dec_cell, loop_fn, swap_memory=True)\n",
    "    sample = outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = tf.placeholder_with_default(LEARNING_RATE, [], name=\"lr\")\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(ave_loss, tvars), 1.0)\n",
    "optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Variables:\n",
      "====================\n",
      "           [95, 128]    12160 embedding:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "          [1024, 95]    97280 decoder/rnn/output_projection_wrapper/weights:0\n",
      "                [95]       95 decoder/rnn/output_projection_wrapper/biases:0\n",
      "Total trainable parameters: 43125727\n",
      "\n",
      "Other Varaibles:\n",
      "================\n",
      "           [95, 128]    12160 embedding/RMSProp:0\n",
      "           [95, 128]    12160 embedding/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "          [1024, 95]    97280 decoder/rnn/output_projection_wrapper/weights/RMSProp:0\n",
      "          [1024, 95]    97280 decoder/rnn/output_projection_wrapper/weights/RMSProp_1:0\n",
      "                [95]       95 decoder/rnn/output_projection_wrapper/biases/RMSProp:0\n",
      "                [95]       95 decoder/rnn/output_projection_wrapper/biases/RMSProp_1:0\n",
      "Total non-trainable parameters: 86251454\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def print_shapes():\n",
    "    train_vars = tf.trainable_variables()\n",
    "    \n",
    "    lines = ['']\n",
    "    lines.append('Trainable Variables:')\n",
    "    lines.append('====================')\n",
    "    total_params = 0\n",
    "    for var in train_vars:\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total trainable parameters: %d' % total_params)\n",
    "    \n",
    "    lines.append('')\n",
    "    lines.append('Other Varaibles:')\n",
    "    lines.append('================')\n",
    "    total_params = 0\n",
    "    for var in tf.global_variables():\n",
    "        if var in train_vars: continue\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total non-trainable parameters: %d' % total_params)\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print(print_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#coord.join(threads)\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = './checkpoints/'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "latest_checkpoint = tf.train.latest_checkpoint(CHECKPOINT_PATH)\n",
    "if latest_checkpoint:\n",
    "    saver.restore(sess, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "227.666320801 ( 6.56906143993 )\t| 59.208 sec\n",
      "Such killings are first degree murder plain and si --> A\":\"-k'gA\"{;jSUT(]6:%RpW~ooLz(9&k:,)t3_UXkq}W'JqR@\n",
      "Fox News is actually owned by 21st Century Fox, a  --> tdP-5+4;x(~l~qo{6Z3]TR<#l?&!#_%kCqP^%Hd(dt_S93mgw\n",
      "Nathan and his pal Josh want to bang.              --> ;N9_*v9~OGTZ4iyTo<?hpFY^?Ib/\"::MasX(E&=Ru;!^uq%1NI\n",
      "Such killings are first degree murder plain and si --> 0Ko+00^Wo4~!K]`7dugseaX%5[,)n?!;WDK9SlUR!#!WQB\"mX\n",
      "OMG yes!!! could you imagine being face to face wi --> ;2hux%{u0r_M$D6mRw\"Q6]J=,`R,7-~SEK7(NfAsw3UP&CGLU1\n",
      "OMG yes!!! could you imagine being face to face wi --> +pNk.6s}r}Ijy&@0gYK2MxchQn_r@u<Z%06zy*l^(kZBL\"P,A\n",
      "I'd be willing to bet that bethesda doesn't like b --> _S2Rr?Ya#9o,bE[:^y>3A4?2]Wob?WzrAb8^NdbN!W\"54\n",
      "I'd be willing to bet that bethesda doesn't like b --> N*>1J&'(%<'twYzg^]z:]*PY'%RIg*7w`<>`h:3I`,\"U}z4>>>\n",
      "I'd be willing to bet that bethesda doesn't like b --> h*a\\[&SY>5!wolKhq0eh]!Bvw5F(Tpcu+]+F45:HBeJ`Oj#y3o\n",
      "Nathan and his pal Josh want to bang.              --> ](q27W~Q'<5s3>WG:Ku93B{@4KsP>r/3&?}S.?ZS0Wg*KPZZ.c\n",
      "Fox News is actually owned by 21st Century Fox, a  --> RhLYQ_/NDI0'vH>nCE\\Uk.fxwSw,d&FEPOn15TW.BGw\"NlO~n\n",
      "OMG yes!!! could you imagine being face to face wi --> *BR(q)-pTLMCKpO1uU@ZXb\\uxRi]WFoG{LC{a(AwT5B\"( .\"f\n",
      "OMG yes!!! could you imagine being face to face wi --> j95ql/z$]cEW8@.82GuwdxvW(U3StP_@UQj&rruC@B=s^'gd8_\n",
      "Almost gotta feel a touch bad for Joonas because / --> *[OSz\"dLvt&ht6`<v-A[5]>/g?=2\"Fj(bIf#7zZKwj<i1\"I/Ey\n",
      "Almost gotta feel a touch bad for Joonas because / --> ;oSJJU]4tE;ZqS+Tc;t%upj{5@I{%<^8H( rq`Mf5&eN=XB0\n",
      "OMG yes!!! could you imagine being face to face wi --> $=P LQc_}d7L-LYC?oUk/_ye\"rV1UQ-.-yBeV#[){7CFaXA6a\n",
      "Almost gotta feel a touch bad for Joonas because / --> Rip.p}.W,EO=1V9Ggxp X>s)l&:%Yb'g_*QVtkTjd.b4s*1O\n",
      "Such killings are first degree murder plain and si --> D1vp}t_ }6}16#:LyZytpyHxmyH}:Ck=eT[ZLmu<)CQ@\"mqfux\n",
      "We had issues like this with MAME.  We ended up pa --> \"\"I@J[E}6CE_'Yw/FTDyn.wbO#i&ghi5FhJ)\\9\"]?Bx>mHV\"y<\n",
      "OMG yes!!! could you imagine being face to face wi --> ^pvT-[.Z+Zr3z3W%SnQhzt)2G+q+ueK=9Dor'DmgGkwYxCP{H*\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of /media/llion/Big/reddit_comments/checkpoints/checkpoint doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ef0ab7e0fa57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0ml_ave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_ave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_ave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHECKPOINT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/checkpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m#print('-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m       raise ValueError(\n\u001b[0;32m-> 1354\u001b[0;31m           \"Parent directory of {} doesn't exist, can't save.\".format(save_path))\n\u001b[0m\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of /media/llion/Big/reddit_comments/checkpoints/checkpoint doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "l_ave = b_ave = d_ave = 0\n",
    "\n",
    "UPDATE_EVERY = 100\n",
    "\n",
    "for step in range(500):\n",
    "    start_time = time.time()\n",
    "    _, l = sess.run([train_op, ave_loss], {\n",
    "        lr: 0.0001\n",
    "    })\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    l_ave += l\n",
    "    b_ave += l / SEQ_MAX_LEN / np.log(2.)\n",
    "    d_ave += duration\n",
    "    \n",
    "    #print(\"|\", end=\"\")\n",
    "    if step % UPDATE_EVERY == 0:\n",
    "        print()\n",
    "        l_ave = l_ave / UPDATE_EVERY if step else l_ave\n",
    "        b_ave = b_ave / UPDATE_EVERY if step else b_ave\n",
    "        d_ave = d_ave / UPDATE_EVERY if step else d_ave\n",
    "        \n",
    "        print(step)\n",
    "        print(l_ave, \"(\", b_ave, \")\\t|\", \"%.3f sec\" % d_ave)\n",
    "        c, r = sess.run([comment, sample])\n",
    "        for i in range(20):\n",
    "            print(to_eng(c[i]), \"-->\", to_eng(r[:, i]))\n",
    "\n",
    "        l_ave = b_ave = d_ave = 0\n",
    "        print()\n",
    "        saver.save(sess, CHECKPOINT_PATH + \"/checkpoint\", global_step=0)\n",
    "        #print('-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
