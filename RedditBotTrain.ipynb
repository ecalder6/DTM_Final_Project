{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Neural Bot Trainer\n",
    "-----\n",
    "#### ToDo\n",
    "- Subredding embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_COMMENT_LENGTH = 50\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = \"| abcdefghijklmnopqrstuvwxyz\"+\\\n",
    "        \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"+\\\n",
    "        \"1234567890\"+\\\n",
    "        \"~`!@#$%^&*()_+-=[]{}:;\\\"'<>,./?\\\\\"\n",
    "char_to_ix = { ch:i for i,ch in enumerate(vocab) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(vocab) }\n",
    "\n",
    "def to_eng(ids):\n",
    "    return ''.join([ix_to_char[id] if id != 0 else '' for id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_files = glob.glob('/media/llion/Big/reddit_comments/training_data/*.proto')\n",
    "random.shuffle(proto_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(proto_files)  #num_epochs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(\n",
    "  serialized_example,\n",
    "  # Defaults are not specified since both keys are required.\n",
    "  features={\n",
    "      #'subredddit_id': tf.FixedLenFeature([], tf.int64),\n",
    "      'comment': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "      'replies': tf.FixedLenFeature([MAX_COMMENT_LENGTH], tf.int64),\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_after_dequeue = 10000\n",
    "capacity = min_after_dequeue + 3 * BATCH_SIZE\n",
    "\n",
    "comment, replies = tf.train.shuffle_batch(\n",
    "    [features['comment'], features['replies']],\n",
    "    batch_size=BATCH_SIZE, capacity=capacity, min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "SEQ_MAX_LEN = MAX_COMMENT_LENGTH\n",
    "RNN_HIDDEN_SIZE = 1024\n",
    "LAYERS = 3\n",
    "CHAR_EMB_SIZE = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "#SUBREDDIT_EMB_SIZE = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cell = tf.contrib.rnn.BasicLSTMCell(RNN_HIDDEN_SIZE)\n",
    "enc_cell = tf.contrib.rnn.MultiRNNCell([inner_cell] * LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embeddings = tf.get_variable(\"embedding\", [VOCAB_SIZE, CHAR_EMB_SIZE])\n",
    "emb_comment = tf.nn.embedding_lookup(char_embeddings, comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, thought_vector = tf.nn.dynamic_rnn(\n",
    "    enc_cell, emb_comment, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_input = tf.concat(  # Add GO token to start\n",
    "    [tf.zeros(shape=(BATCH_SIZE, 1), dtype=tf.int64), replies[:, :SEQ_MAX_LEN-1]], axis=1)\n",
    "emb_reply_input = tf.nn.embedding_lookup(char_embeddings, reply_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_cell = tf.contrib.rnn.OutputProjectionWrapper(enc_cell, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\"):\n",
    "    dec_out, _ = tf.nn.dynamic_rnn(\n",
    "        dec_cell, emb_reply_input, initial_state=thought_vector, swap_memory=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=dec_out, labels=replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(xent, axis=[1])\n",
    "ave_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_1/rnn/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/cond/Merge:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/Squeeze:0\", shape=(100,), dtype=int64)\n",
      "Tensor(\"decoder_1/rnn/while/embedding_lookup:0\", shape=(100, 128), dtype=float32)\n",
      "Tensor(\"decoder_1/rnn/while/cond/Merge:0\", shape=(100, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_TEMP = 0.7\n",
    "\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    if cell_output is None:  # time == 0\n",
    "        next_cell_state = thought_vector  # state from the encoder\n",
    "        next_input = tf.zeros([BATCH_SIZE], dtype=tf.int64)  # GO symbol\n",
    "        next_input = tf.nn.embedding_lookup(char_embeddings, next_input)\n",
    "        emit_output = tf.zeros([], dtype=tf.int64)\n",
    "    else:\n",
    "        next_cell_state = cell_state\n",
    "        sample = tf.squeeze(tf.multinomial(cell_output / SAMPLE_TEMP, 1))\n",
    "        print(sample)\n",
    "        emb_sample = tf.nn.embedding_lookup(char_embeddings, sample)\n",
    "        next_input = emb_sample\n",
    "        emit_output = sample\n",
    "    elements_finished = time >= tf.constant(SEQ_MAX_LEN, shape=(BATCH_SIZE,))\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    print(next_input)\n",
    "    next_input = tf.cond(\n",
    "        finished,\n",
    "        lambda: tf.zeros([BATCH_SIZE, CHAR_EMB_SIZE], dtype=tf.float32),\n",
    "        lambda: next_input)\n",
    "    print(next_input)\n",
    "    next_loop_state = None\n",
    "    return elements_finished, next_input, next_cell_state, emit_output, next_loop_state\n",
    "\n",
    "with tf.variable_scope(\"decoder\", reuse=True):\n",
    "    outputs_ta, _, _ = tf.nn.raw_rnn(dec_cell, loop_fn, swap_memory=True)\n",
    "    sample = outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.placeholder_with_default(LEARNING_RATE, [], name=\"lr\")\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(ave_loss, tvars), 1.0)\n",
    "optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Variables:\n",
      "====================\n",
      "           [95, 128]    12160 embedding:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases:0\n",
      "          [1024, 95]    97280 decoder/rnn/output_projection_wrapper/weights:0\n",
      "                [95]       95 decoder/rnn/output_projection_wrapper/biases:0\n",
      "Total trainable parameters: 43125727\n",
      "\n",
      "Other Varaibles:\n",
      "================\n",
      "           [95, 128]    12160 embedding/RMSProp:0\n",
      "           [95, 128]    12160 embedding/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp:0\n",
      "        [1152, 4096]  4718592 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/RMSProp_1:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp:0\n",
      "        [2048, 4096]  8388608 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/weights/RMSProp_1:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp:0\n",
      "              [4096]     4096 decoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/biases/RMSProp_1:0\n",
      "          [1024, 95]    97280 decoder/rnn/output_projection_wrapper/weights/RMSProp:0\n",
      "          [1024, 95]    97280 decoder/rnn/output_projection_wrapper/weights/RMSProp_1:0\n",
      "                [95]       95 decoder/rnn/output_projection_wrapper/biases/RMSProp:0\n",
      "                [95]       95 decoder/rnn/output_projection_wrapper/biases/RMSProp_1:0\n",
      "Total non-trainable parameters: 86251454\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def print_shapes():\n",
    "    train_vars = tf.trainable_variables()\n",
    "    \n",
    "    lines = ['']\n",
    "    lines.append('Trainable Variables:')\n",
    "    lines.append('====================')\n",
    "    total_params = 0\n",
    "    for var in train_vars:\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total trainable parameters: %d' % total_params)\n",
    "    \n",
    "    lines.append('')\n",
    "    lines.append('Other Varaibles:')\n",
    "    lines.append('================')\n",
    "    total_params = 0\n",
    "    for var in tf.global_variables():\n",
    "        if var in train_vars: continue\n",
    "        n_param = reduce(operator.mul, var.get_shape().as_list(), 1)\n",
    "        total_params += n_param\n",
    "        lines.append('%20s %8d %s' % (var.get_shape().as_list(), n_param, var.name))\n",
    "    lines.append('Total non-trainable parameters: %d' % total_params)\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print(print_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#coord.join(threads)\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = '/media/llion/Big/reddit_comments/checkpoints'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "latest_checkpoint = tf.train.latest_checkpoint(CHECKPOINT_PATH)\n",
    "if latest_checkpoint:\n",
    "    saver.restore(sess, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "68.1528625488 ( 1.96647593643 )\t| 4.520 sec\n",
      "Shit, I break a sweat just reading \"FIRST POST!!!\" --> Ene some the an rete I be a the come to was here s\n",
      "put 7 on DEX and the rest on STR --> Chat the sely the hard the candy coumt.\n",
      "OH FUCK YOU   IM DONE WITH THIS SHIT WEBSITE --> Thagt that Gus an a chat of lew ame here.\n",
      "Author best ideas have already been done. --> Cleal he seirre firts right of EaBes aper of bay.\n",
      "You guys are nuts. --> I'm pay that rets lan the epery!\n",
      "Oh crap, that's what I was bidding on? --> I let on Do ving me stape this me the kis the dric\n",
      "Nobody wants to see that. --> It's me it ars achasionamine you jait thhe spiees.\n",
      "I c what you did there. --> I was the 4* Wall his aring that a not that.\n",
      "Still got my greatest treasure --> what sparst now it the she cormends.\n",
      "DRINK ME --> Sele hat of the the and madiin hen thore cher lrab\n",
      "My god, an organization of grammar nazis! --> no but and hore bew that me she a sho this what a \n",
      "**Where is the red button?!** --> Hey same suck to shat's and that?\n",
      "It felt good to be spoken to BY an adult. --> Not the git for the prees what the som cof\n",
      "cunt pasties sound hairy and disgusting. --> Oht she have a staly it In't chat it you a time th\n",
      "stealing our precious fluids! --> She you erpy couldly?\n",
      "MUVIT! --> Ne was lay stad spoppter cort.\n",
      "Hmm, you're a user... --> It lime the calca ucal AVCFTFAF WAD FA SEERE!\n",
      "anonymous is created by Scientology --> peched this got of in he mich the puint a dait.\n",
      "doesn't faze me, bro --> Not the steplsy the rood to do labl chaning it roa\n",
      "http://octogenarian-election-fraud-squad.com/ --> Uth should shat that mere celle.\n",
      "\n",
      "\n",
      "100\n",
      "64.6460824966 ( 1.86529165262 )\t| 0.629 sec\n",
      "Ironic that the \"Air\" keeps him grounded. --> Ind amwate is a heating the propcine?\n",
      "i have web-tv --> I dow'h she syingerss of the Sluss I.\n",
      "NICE TO EAT YOU. --> AAM ACS MICT LOT BINTH I NOT BONK YOT TUULK WISSAT\n",
      "Jeans that say \"JUICY\" on the ass. --> Ol coull ferling bust a not the here.\n",
      "73% of statistics are made up on the spot --> I'm and not the word whelk has a mashis.\n",
      "Threats only seem to make Wikileaks stronger. --> They up sto it ank had of 1 \\FF\n",
      "May God have mercy on their Seoul. --> Not what your about.  harded...\n",
      "President Barack Obama's penis. --> They has casters he paght rapclest\n",
      "*Gives AtheismFTW a melon* --> UCK  cond fucking ase a F80 2 N fuck don't did.\n",
      "He ought to hand in his badge, the craven coward! --> he comestalle came.  Prest of the comeronys.\n",
      "Sucks to be that crab. --> Seave of a the was you's ally where the ded.\n",
      "Zerg Rush! --> That's bott where you're sanverfish.\n",
      "I'm sure she can convert my noodly appendage. --> *pack* me *zich**?\n",
      "And that's how memes die. --> CHY UFC HR THE  HILT SAALK NET AW IN FOIST CHITS\n",
      "She was worth my Sword of Thunder! --> Annethis, soment: the sarrist.\n",
      "I would be sad too if someone ate my cake. --> That's ale was a seach?\n",
      "mostly? --> The sthe wish what for the doy.\n",
      "Hank Hill won't like this one. --> He don't this what bet\n",
      "http://en.wikipedia.org/wiki/Joking --> You and a maysher, dendy.\n",
      "What will the last post on Reddit be, I wonder? --> I heard stapilets at fom for of the that.\n",
      "\n",
      "\n",
      "200\n",
      "62.3397927094 ( 1.79874619584 )\t| 0.625 sec\n",
      "I would have started hitting on the reporter. --> It's shat thane joken the more to sprofer us Ind.\n",
      "Could someone tell me what this says? --> The best pay Hergiag\n",
      "Hello, Jerry! --> I don't shere to shat one a cay to the on got.\n",
      "[This is heavy shit](http://imgur.com/MCQB1.jpg) --> Nout a but it best sarite is it wat the sore for c\n",
      "TAKE  OWN  ADVICE --> MENASEINE!\n",
      "Happy beer is also 2-face? --> I'm a bat to f Hir this trit of thy enours.\n",
      "these were all terrible. --> Why you dit wore do Serilling are the erit...\n",
      "Sorry, this can only be done on Linux. --> It's have think of Sto do.\n",
      "And reddit ruins Christmas --> * A gid the Can is to the with it.\n",
      "relax, you'll live longer. --> Hey don't do so did.\n",
      "# Internet Fad Collides With Real World # --> NE chantry actuiling to stapter of encar.\n",
      "We said nothing about dinosaur-zombies. --> Giges that the his a bees of therare spoundis.\n",
      "Cream me up, Scotty. --> Ant that a bist to with the peution it wor the bic\n",
      "i wish i had a fuck buddy, or a relationship... :( --> And you gray a call of the are to they ara....\n",
      "So that's all of em...    Bummer! --> And I call ot lith strote for eserite in be sup.\n",
      "Hey heyyy --> Thit I this 'nime is you gat the mare!\n",
      "What if his mom's hot, single, 1337, and cheap? --> I don't pan rath snot so thread.\n",
      "THIRD! --> I this not what a pan are the wam whit in.\n",
      "Where's the subject of your sentence? --> And a sure the to be a cot exter is a got.\n",
      "Sturmbannfhrer Heinrich Livestrongbracelet --> Ame, that's is a suct of the storn.\n",
      "\n",
      "\n",
      "300\n",
      "60.6543136978 ( 1.75011355161 )\t| 0.633 sec\n",
      "Awwwwwwwww. --> Guit I samn for dight the was out thene.\n",
      "IT'S A TRAP! --> I'm *HOL HOS A HOT I IL ABUUCUM\n",
      "Woooo! --> No don't to bust on didsting there.\n",
      "Visual Basic Ct is a dead end. --> No, it's me to bad the leddit the to proond were o\n",
      "Illinois drivers, all the time. --> he's can me with a blitc.\n",
      "yeah, you know what? fuck you!  edit: ;) --> Yeap way can frich night that the bet arred.\n",
      "President's black, racism's over. I saw the memo. --> No, that's we merlagly :P\n",
      "What if you wanted them to taste like clouds? --> Well perient the seming and on hear belord.\n",
      "A heart that Steve Jobs no longer has. :P --> And the bund can twe see mare canding peats.\n",
      "He should just submit directly to bestof. --> Ind it denditally ade the on saz this?\n",
      "Sex.  It's only like $40 where I go. --> Thanks whe everscoods less try.\n",
      "1. Be nice. --> I'm a fursiand the preanem in herd cound\n",
      "You had *me* at Natalie Portman. --> I lid thing did the you the ded ant of the is.\n",
      "[OBJECTION!](http://imgur.com/btZn3.jpg) --> I that hand you stex tell the mack.\n",
      "Tao, yo. --> be was the ned the bed the bead ay has say to tes.\n",
      "Agreed. Seems like a complete fabrication to me. --> Srulbid  gugs to did.\n",
      "and The Matrix is ten years old!? --> Nod you hore it a sack the bow in this?\n",
      "I'd hit that. --> Book did the not didn't stred it the round it.\n",
      "or at the country club --> .... net is just all in that's need the retit ham \n",
      "Actually, legal home brew games. --> Now you aret well sepliting cremples!\n",
      "\n",
      "\n",
      "400\n",
      "59.5407242584 ( 1.71798215237 )\t| 0.639 sec\n",
      "That's swell. --> I have read end.\n",
      "you didn't see the sister... --> I sead him bale.\n",
      "Wow. Any single blind guys free tonight? --> Hey seera he for mere as and and shew was\n",
      "Your **AND TOES**? --> Fured fack fuck.\n",
      "He bozarked *her* --> a reddit I meen Parly on the lusg.\n",
      "Well, he does regular acting. --> Which the canber seejuce.\n",
      "Bland... I mean Anne. --> Killly I was think that down.\n",
      "tineye.com'll cure what ails ya! --> *arr night.\n",
      "Remember ALF!? Well he's back... in pog form. --> Shit cake the said it unding of she same right.\n",
      "Johnathan Coulton? Why should he be in whalebait? --> Fird you leal.\n",
      "haha, you should have married that girl. :) --> And I past a tour the tern Darba Kidding.\n",
      "That's a bingo! --> Bold the lest that out the aurd there.\n",
      "yes.... \"hilarity\" --> Goldaadn't get to gill\n",
      "I like karma. --> Or you gick\n",
      "Nice try Burger King --> Coller packsang\n",
      "They mostly come at night.... --> He secy and of the comment be the shard and.\n",
      "That's not a haiku. --> Light and It's at reass.\n",
      "touche. --> That's one the fore me swell the rover and and Beg\n",
      "It's because superman gave him a VHS. --> that's a sure as the say.\n",
      "They didn't even get to use the crowbar. --> Put yway buck I like for defiend the?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l_ave = b_ave = d_ave = 0\n",
    "\n",
    "UPDATE_EVERY = 100\n",
    "\n",
    "for step in range(500):\n",
    "    start_time = time.time()\n",
    "    _, l = sess.run([train_op, ave_loss], {\n",
    "        lr: 0.0001\n",
    "    })\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    l_ave += l\n",
    "    b_ave += l / SEQ_MAX_LEN / np.log(2.)\n",
    "    d_ave += duration\n",
    "    \n",
    "    #print(\"|\", end=\"\")\n",
    "    if step % UPDATE_EVERY == 0:\n",
    "        print()\n",
    "        l_ave = l_ave / UPDATE_EVERY if step else l_ave\n",
    "        b_ave = b_ave / UPDATE_EVERY if step else b_ave\n",
    "        d_ave = d_ave / UPDATE_EVERY if step else d_ave\n",
    "        \n",
    "        print(step)\n",
    "        print(l_ave, \"(\", b_ave, \")\\t|\", \"%.3f sec\" % d_ave)\n",
    "        c, r = sess.run([comment, sample])\n",
    "        for i in range(20):\n",
    "            print(to_eng(c[i]), \"-->\", to_eng(r[:, i]))\n",
    "\n",
    "        l_ave = b_ave = d_ave = 0\n",
    "        print()\n",
    "        saver.save(sess, CHECKPOINT_PATH + \"/checkpoint\", global_step=0)\n",
    "        #print('-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|' + '-'*24 + '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
